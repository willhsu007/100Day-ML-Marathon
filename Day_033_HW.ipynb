{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請觀看李宏毅教授以神奇寶貝進化 CP 值預測的範例，解說何謂機器學習與過擬合。並回答以下問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[youtube](https://www.youtube.com/watch?v=fegAeph9UaA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模型的泛化能力 (generalization) 是指什麼？ \n",
    "### 2. 分類問題與回歸問題分別可用的目標函數有哪些？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 模型的泛化能力 (generalization) 是指什麼？\n",
    "定義:\n",
    "概括地說，所謂泛化能力(generalization ability)是指機器學習演算法對新鮮樣本的適應能力。\n",
    "學習的目的是學到隱含在資料對背後的規律，對具有同一規律的學習集以外的資料，\n",
    "經過訓練的網路也能給出合適的輸出，該能力稱為泛化能力。 \n",
    "性質\n",
    "通常期望經訓練樣本訓練的網路具有較強的泛化能力，也就是對新輸入給出合理回響的能力。\n",
    "應當指出並非訓練的次數越多越能得到正確的輸入輸出對應關系。\n",
    "網路的性能主要用它的泛化能力來衡量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. 分類問題與回歸問題分別可用的目標函數有哪些？\n",
    "在機器學習中，所有的機器學習算法都或多或少的依賴於對目標函數最大化或者最小化的過程，\n",
    "我們常常把最小化的函數稱爲損失函數，它主要用於衡量機器學習模型的預測能力。\n",
    "迴歸會預測給出一個數值結果而分類則會給出一個標籤。最小二乘法是損失函數的一種優化方法。\n",
    "但是有些損失函數採用傳統數學方法是很難求解最小值的，而機器學習中的反向傳播算法爲求解極\n",
    "值點提供了一種簡單有效的途徑，從而我們只需要把重點放在損失函數的自身特性上，\n",
    "而不用花過多心思在函數求解的方法上。\n",
    "機器學習分爲四大塊，分別是 \n",
    "classification (分類)， \n",
    "regression (迴歸), \n",
    "clustering (聚類),  \n",
    "dimensionality reduction (降維)。\n",
    "1.迴歸問題[1]\n",
    "迴歸問題中有衆多損失函數，而目前機器學習主流的損失函數還是均方誤差函數，\n",
    "平均絕對誤差也使用較多，在此基礎上發展出了很多其他函數，Huber損失函數就是其中一種。\n",
    "1.1平均絕對誤差——L1損失函數\n",
    "平均絕對誤差（MAE）是另一種常用的迴歸損失函數，它是目標值與預測值之差絕對值的和，\n",
    "表示了預測值的平均誤差幅度，而不需要考慮誤差的方向，範圍是0到∞，\n",
    "其中代表真實值，代表預測結果，兩者都是向量。因爲絕對誤差相當於對誤差求L1範數，\n",
    "所以也稱爲L1損失函數（L1 loss）。\n",
    "1.2均方誤差——L2損失函數\n",
    "均方誤差（MSE）是迴歸損失函數中最常用的誤差，它是預測值與目標值之間差值的平方和，\n",
    "公式與L1損失函數區別在於多了一步平方計算，等價於對誤差求L2範數，\n",
    "所以也叫L2損失函數（L2 loss）。\n",
    "1.3 Huber損失——平滑平均絕對誤差\n",
    "Huber損失相比於平方損失來說對於異常值不敏感，但它同樣保持了可微的特性。\n",
    "它基於絕對誤差，但在誤差很小的時候變成了平方誤差。\n",
    "我們可以使用超參數δ來調節這一誤差的閾值。當δ趨向於0時它就退化成了MAE，\n",
    "而當δ趨向於無窮時則退化爲了MSE，其表達式如下，是一個連續可微的分段函數：\n",
    "\n",
    "2.分類問題\n",
    "分類問題相對迴歸問題更爲具體，目標量只存在於一個有限集合，並且是離散的。\n",
    "分類問題往往比迴歸問題多出了一步，用於判斷類別。迴歸問題的損失函數就是性能度量函數，\n",
    "而分類問題的損失函數不能直接用於性能度量，其最終評估的標準不是離目標的距離，\n",
    "而是類別判斷的準確率。爲了最大地提升類別判斷準確率，我們需要爲分類問題定義不同的損失函數。\n",
    "2.1  0-1損失函數\n",
    "以二分類問題爲例，錯誤率=1-正確率，也就是0-1損失函數，可以定義爲\n",
    "2.2 交叉熵損失函數（Logistic迴歸）[6]\n",
    "從0-1損失函數得出，爲了便於優化，我們需要連續的損失函數，因而可以用期望風險代替經驗風險，\n",
    "並且上文已經說明對期望風險的優化就是對經驗風險的優化。\n",
    "2.3 交叉熵損失函數（Softmax激活）\n",
    "交叉熵用到了log函數的特性，它不改變凸函數的凹凸性，同時能簡化計算，是常用的技巧。\n",
    "這裏主要講多分類問題，激活函數是softmax，作用在神經網絡的最後一層，\n",
    "損失函數是對數似然函數，也可以看成是交叉熵損失函數。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
